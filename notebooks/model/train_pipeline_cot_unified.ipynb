{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/u/shuhan/projects/vla')\n",
    "\n",
    "from src.environments.highway_env.dataset import HighwayDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_folder = '/storage/Datasets/highway_env/highway_fast_v0_dqn_meta_action_5_lanes/rollouts_train'\n",
    "dataset = HighwayDataset(data_folder)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 5, 5]) torch.Size([2, 30]) torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "obs, act, valid_mask = batch\n",
    "print(obs.shape, act.shape, valid_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load GPT-2 \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = 'HuggingFaceTB/SmolLM2-135M-Instruct'\n",
    "hidden_dim = 576\n",
    "\n",
    "# checkpoint = 'gpt2'\n",
    "# hidden_dim = 768\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.auto_labeling.highway_env.lane_change import LaneChangeTaskSpec\n",
    "from src.models.vlas.cont_obs_token_action_cot_unified_token import ContObsTokenActionCOTVLAUnifiedToken\n",
    "\n",
    "llm_backbone = model\n",
    "task_spec_func = LaneChangeTaskSpec\n",
    "\n",
    "obs_dim = 25\n",
    "num_actions = 5\n",
    "mlp_layers = 2\n",
    "\n",
    "loss_weight = {'action': 1.0, 'reconst': 1.0, 'cot': 1.0, 'separator': 1.0, 'rollout_stop': 1.0}\n",
    "cot_cfg = {'lanes_count': 5, 'max_hop': 4, 'cot_index_mode': 'both'}\n",
    "\n",
    "cot_mode = 'all'\n",
    "# cot_mode = 'start'\n",
    "# cot_mode = 'none'\n",
    "\n",
    "vla = ContObsTokenActionCOTVLAUnifiedToken(llm_backbone, tokenizer, task_spec_func, obs_dim, num_actions, hidden_dim, mlp_layers, loss_weight, cot_mode, cot_cfg, max_obs_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 5, 5])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict, batch_input_embeds = vla.forward(obs, act, valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'separator': tensor(28.5757, grad_fn=<MeanBackward0>),\n",
       " 'rollout_stop': tensor(30.0580, grad_fn=<MeanBackward0>),\n",
       " 'action': tensor(27.6343, grad_fn=<MeanBackward0>),\n",
       " 'cot': tensor(0.9193, grad_fn=<MeanBackward0>),\n",
       " 'reconst': tensor(0.2950, grad_fn=<DivBackward0>),\n",
       " 'total': tensor(87.4823, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_flat_labels['rollout_stop']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the batch seq_emds with padding\n",
    "# record validity mask for each sequence\n",
    "batch_seq_emds_flat = torch.nn.utils.rnn.pad_sequence(batch_seq_emds, batch_first=True)\n",
    "batch_seq_emds_valid = torch.zeros_like(batch_seq_emds_flat[..., 0], dtype=torch.bool)\n",
    "for bidx in range(len(batch_seq_emds)):\n",
    "  batch_seq_emds_valid[bidx, :batch_seq_emds[bidx].shape[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_flat_labels = {type_name: {} for type_name in batch_index_labels[0].keys()}\n",
    "\n",
    "# flatten action labels\n",
    "action_labels = batch_index_labels[0]['act']\n",
    "action_bidx_list = [torch.tensor(bidx).repeat(len(batch_index_labels[bidx]['act']['index'])) for bidx in range(len(batch_index_labels))]\n",
    "batch_flat_labels['act']['bidx'] = torch.cat(action_bidx_list, dim=0)\n",
    "batch_flat_labels['act']['tidx'] = torch.cat([torch.tensor(index_labels['act']['index']) for index_labels in batch_index_labels], dim=0)\n",
    "batch_flat_labels['act']['label'] = torch.cat([torch.tensor(index_labels['act']['label']) for index_labels in batch_index_labels], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten cot labels\n",
    "cot_index_flat, cot_label_flat = [], []\n",
    "\n",
    "for bidx in range(len(batch_index_labels)):\n",
    "  cot_index_flat.append(torch.cat(batch_index_labels[bidx]['cot']['index'], dim=0))\n",
    "  cot_label_flat.append(torch.cat(batch_index_labels[bidx]['cot']['label'], dim=1)[0])\n",
    "\n",
    "cot_bidx_list = [torch.tensor(bidx).repeat(len(cot_index_flat[bidx])) for bidx in range(len(batch_index_labels))]\n",
    "batch_flat_labels['cot']['bidx'] = torch.cat(cot_bidx_list, dim=0)\n",
    "batch_flat_labels['cot']['tidx'] = torch.cat(cot_index_flat, dim=0)\n",
    "batch_flat_labels['cot']['label'] = torch.cat(cot_label_flat, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten cot_cls and rollout_stop labels\n",
    "for cls_type in ['cot_cls', 'rollout_stop']:\n",
    "  cls_index_flat, cls_label_flat = [], []\n",
    "  for bidx in range(len(batch_index_labels)):\n",
    "    cls_index_flat.append(torch.tensor(batch_index_labels[bidx][cls_type]['index']))\n",
    "    cls_label_flat.append(torch.tensor(batch_index_labels[bidx][cls_type]['label'], dtype=torch.long))\n",
    "  cls_bidx_list = [torch.tensor(bidx).repeat(len(cls_index_flat[bidx])) for bidx in range(len(batch_index_labels))]\n",
    "  batch_flat_labels[cls_type]['bidx'] = torch.cat(cls_bidx_list, dim=0)\n",
    "  batch_flat_labels[cls_type]['tidx'] = torch.cat(cls_index_flat, dim=0)\n",
    "  batch_flat_labels[cls_type]['label'] = torch.cat(cls_label_flat, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 3844,   379, 15016,   604,    13,  7281, 15016,   513,  4613, 15016,\n",
       "            362,  4613, 15016,   352,  4613, 15016,   362,    13,  7406,   318,\n",
       "          15016,   513,    13,  7561,    25,  1210,  1364,    13, 50257]]),\n",
       " tensor([[ 3844,   379, 15016,   604,    13,  7281, 15016,   513,  4613, 15016,\n",
       "            362,  4613, 15016,   352,  4613, 15016,   362,    13,  7406,   318,\n",
       "          15016,   513,    13,  7561,    25,  1210,  1364,    13, 50257]]),\n",
       " tensor([[ 3844,   379, 15016,   513,    13,  7281, 15016,   362,  4613, 15016,\n",
       "            352,  4613, 15016,   362,    13,  7406,   318, 15016,   362,    13,\n",
       "           7561,    25,  1210,  1364,    13, 50257]]),\n",
       " tensor([[ 3844,   379, 15016,   362,    13,  7281, 15016,   352,  4613, 15016,\n",
       "            362,    13,  7406,   318, 15016,   352,    13,  7561,    25,  1210,\n",
       "           1364,    13, 50257]]),\n",
       " tensor([[ 3844,   379, 15016,   362,    13,  7281, 15016,   352,  4613, 15016,\n",
       "            362,    13,  7406,   318, 15016,   352,    13,  7561,    25,  1210,\n",
       "           1364,    13, 50257]]),\n",
       " tensor([[ 3844,   379, 15016,   352,    13, 25376, 25146,   540,    13,  7406,\n",
       "            318, 15016,   362,    13,  7561,    25,  1210,   826,    13, 50257]]),\n",
       " tensor([[ 3844,   379, 15016,   352,    13, 25376, 25146,   540,    13,  7406,\n",
       "            318, 15016,   362,    13,  7561,    25,  1210,   826,    13, 50257]])]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index_labels[bidx]['cot']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
       "         48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]),\n",
       " tensor([120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
       "         134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
       "         148]),\n",
       " tensor([156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "         170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181]),\n",
       " tensor([189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n",
       "         203, 204, 205, 206, 207, 208, 209, 210, 211]),\n",
       " tensor([219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "         233, 234, 235, 236, 237, 238, 239, 240, 241]),\n",
       " tensor([249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262,\n",
       "         263, 264, 265, 266, 267, 268]),\n",
       " tensor([282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "         296, 297, 298, 299, 300, 301])]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index_labels[bidx]['cot']['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_bidx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 54,  60,  66,  99, 129, 135, 141, 171, 198, 204, 210, 216, 222, 228,\n",
       "        234, 240, 246, 252, 258, 264, 270, 276, 282, 288, 294, 300, 306, 333])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(batch_index_labels[0]['act']['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 2, 1, 3, 1, 2, 1, 1, 2, 1, 4, 1, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(batch_index_labels[0]['act']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54,\n",
       " 60,\n",
       " 66,\n",
       " 99,\n",
       " 129,\n",
       " 135,\n",
       " 141,\n",
       " 171,\n",
       " 198,\n",
       " 204,\n",
       " 210,\n",
       " 216,\n",
       " 222,\n",
       " 228,\n",
       " 234,\n",
       " 240,\n",
       " 246,\n",
       " 252,\n",
       " 258,\n",
       " 264,\n",
       " 270,\n",
       " 276,\n",
       " 282,\n",
       " 288,\n",
       " 294,\n",
       " 300,\n",
       " 306,\n",
       " 333]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index_labels[0]['act']['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 59,\n",
       " 65,\n",
       " 71,\n",
       " 104,\n",
       " 134,\n",
       " 140,\n",
       " 146,\n",
       " 176,\n",
       " 203,\n",
       " 209,\n",
       " 215,\n",
       " 221,\n",
       " 227,\n",
       " 233,\n",
       " 239,\n",
       " 245,\n",
       " 251,\n",
       " 257,\n",
       " 263,\n",
       " 269,\n",
       " 275,\n",
       " 281,\n",
       " 287,\n",
       " 293,\n",
       " 299,\n",
       " 305,\n",
       " 311]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index_labels[0]['cot_cls']['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
       "         45, 46, 47, 48, 49, 50, 51, 52]),\n",
       " tensor([72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "         90, 91, 92, 93, 94, 95, 96, 97]),\n",
       " tensor([105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "         119, 120, 121, 122, 123, 124, 125, 126, 127]),\n",
       " tensor([147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160,\n",
       "         161, 162, 163, 164, 165, 166, 167, 168, 169]),\n",
       " tensor([177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190,\n",
       "         191, 192, 193, 194, 195, 196]),\n",
       " tensor([312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "         326, 327, 328, 329, 330, 331])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index_labels[0]['cot']['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([339, 768])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq_emds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309, 768])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq_emds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goal is to reach Lane 2. Need to go through path Lane 4 -> Lane 3 -> Lane 2 -> Lane 1 -> Lane 2. [BOO] <Obs_0> [EOO] [BOT] Now at Lane 4. Follow Lane 3 -> Lane 2 -> Lane 1 -> Lane 2. Next is Lane 3. Action: turn left.[EOT] [BOA] <Act_0> [EOA] [BOO] <Obs_1> [EOO] [BOA] <Act_1> [EOA] [BOO] <Obs_2> [EOO] [BOA] <Act_2> [EOA] [BOO] <Obs_3> [EOO] [BOA] <Act_3> [EOA] [BOO] <Obs_4> [EOO] [BOA] <Act_4> [EOA] [BOO] <Obs_5> [EOO] [BOA] <Act_5> [EOA] [BOO] <Obs_6> [EOO] [BOA] <Act_6> [EOA] [BOO] <Obs_7> [EOO] [BOA] <Act_7> [EOA] [BOO] <Obs_8> [EOO] [BOA] <Act_8> [EOA] [BOO] <Obs_9> [EOO] [BOA] <Act_9> [EOA] [BOO] <Obs_10> [EOO] [BOT] Now at Lane 4. Follow Lane 3 -> Lane 2 -> Lane 1 -> Lane 2. Next is Lane 3. Action: turn left.[EOT] [BOA] <Act_10> [EOA] [BOO] <Obs_11> [EOO] [BOT] Now at Lane 3. Follow Lane 2 -> Lane 1 -> Lane 2. Next is Lane 2. Action: turn left.[EOT] [BOA] <Act_11> [EOA] [BOO] <Obs_12> [EOO] [BOT] Now at Lane 2. Follow Lane 1 -> Lane 2. Next is Lane 1. Action: turn left.[EOT] [BOA] <Act_12> [EOA] [BOO] <Obs_13> [EOO] [BOT] Now at Lane 2. Follow Lane 1 -> Lane 2. Next is Lane 1. Action: turn left.[EOT] [BOA] <Act_13> [EOA] [BOO] <Obs_14> [EOO] [BOT] Now at Lane 1. Goal Reachable. Next is Lane 2. Action: turn right.[EOT] [BOA] <Act_14> [EOA] [BOO] <Obs_15> [EOO] [BOA] <Act_15> [EOA] [BOO] <Obs_16> [EOO] [BOT] Now at Lane 1. Goal Reachable. Next is Lane 2. Action: turn right.[EOT] [BOA] <Act_16> [EOA] [BOO] <Obs_17> [EOO] [EndOfRollout]'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_preview_strs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50257]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla.llm_tokenizer.encode('[EOT]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, loss_dict = vla(obs, act, valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': tensor(1.6000, grad_fn=<DivBackward0>),\n",
       " 'obs': tensor(0.8963, grad_fn=<DivBackward0>),\n",
       " 'reconst': tensor(0.2941, grad_fn=<DivBackward0>),\n",
       " 'total': tensor(2.7904, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict['total'].backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
