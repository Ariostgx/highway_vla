# @package _global_
# Base experiment configuration - equivalent to BaseExperiment.DEFAULTS

# Model weights
action_weight: 1.0
reconst_weight: 1.0
cot_weight: 1.0
separator_weight: 1.0
rollout_stop_weight: 1.0
wm_weight: 1.0

# Training configuration
action_sample_mode: 'future'  # 'random', 'future'
safe_reflect_rate: 0.2
collide_reflect_rate: 0.8
collide_rewind_rate: 0.8
max_rewind_step: 1
shortest_seq_rate: 0.0

# Model settings
use_wm: false
exp_name: 'test'
llm_model: 'SmolLM2-135M-Instruct'  # 'gpt2', 'SmolLM2-135M-Instruct'
overfit: false
mask_collision_action: false
ckpt_path: ""

# Training hyperparameters
lr: 1e-3
lr_scheduler: 'cosine'
batch_size: 12
gradient_accumulation_steps: 1
num_epochs: 5
warmup_steps: 100
num_workers: 4
save_steps: 10000
rollout_steps: 20000
max_token_num: 512
log_freq: 20
loss_clip: 1.0
optimizer: 'adamw'

# Training control
single_gpu: false
fp16: false
torch_compile: false
always_from_scratch: false
rollout_only: false 