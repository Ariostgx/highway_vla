# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 12 --max_token_num 512 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_accum_5_cosine --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 5

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 12 --max_token_num 512 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_1.0_grad_clip_const_accum_5_cosine --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 1.0 --gradient_accumulation_steps 5

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 12 --max_token_num 512 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_accum_10_cosine --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 10

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 12 --max_token_num 512 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_accum_5_cosine_warmup_10000 --warmup_steps 10000 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 5

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/7gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_accum_5_cosine_360M --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 5


# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_bz_240_cosine_360M --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 10 --num_epochs 30

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_1.0_grad_clip_const_bz_240_cosine_360M --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 1.0 --gradient_accumulation_steps 10 --num_epochs 30

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_bz_240_cosine_360M_warmup_10000 --llm_model SmolLM2-360M-Instruct --warmup_steps 10000 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 10 --num_epochs 30

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_bz_1200_cosine_360M --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 50 --num_epochs 30

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_bz_240_cosine_360M_shortest_seq_rate_0.5 --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 10 --num_epochs 30 --shortest_seq_rate 0.5

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_no_grad_clip_const_bz_240_cosine_360M_shortest_seq_rate_0.2 --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 0.0 --gradient_accumulation_steps 10 --num_epochs 30 --shortest_seq_rate 0.2

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_1.0_grad_clip_const_bz_1200_cosine_360M --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 1.0 --gradient_accumulation_steps 50 --num_epochs 30

# accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_1.0_grad_clip_const_bz_240_cosine_360M_shortest_seq_rate_0.5 --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 1.0 --gradient_accumulation_steps 10 --num_epochs 30 --shortest_seq_rate 0.5

accelerate launch --config_file /u/shuhan/projects/vla/scripts/accl_train/4gpu.yaml /u/shuhan/projects/vla/src/training/train.py --base_cfg with_wm_rewind_4 --batch_size 6 --max_token_num 468 --exp_name with_wm_cr_0.8_re_0.8_sr_0.2_max_rewind_step_4_adam_1.0_grad_clip_const_bz_240_cosine_360M_shortest_seq_rate_0.2 --llm_model SmolLM2-360M-Instruct --warmup_steps 0 --optimizer adam --lr_scheduler cosine --loss_clip 1.0 --gradient_accumulation_steps 10 --num_epochs 30 --shortest_seq_rate 0.2